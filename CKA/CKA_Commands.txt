
Important commands for Kubernetes:

To run a Pod :

	kubectl run <podname> --image=<Imagename>
  
To run a deployment :

	kubectl create deployment <name> --image=<imagename> --replicas=<no>

Expose deployment on a port

	kubectl expose deployment nginx --port 80
	
Edit deployment

	kubectl edit deployment <name>
	
Scale deployment

	kubectl scale deployment <name> --replicas=4
	
update the Image

	kubectl set Image deployment <name> <oldImage>=<image>


To Taint a Node :
	
	kubectl taint node <nodeName> <key>=<value>:<taint-effect>
												
												taint-effect = NoSchedule| PreferNoSchedule | NoExecute
	
	ex: kubectl taint node node01 app=frontEnd:NoSchedule
	
To remove Taint on a Node :

	Add - at the end of taint command
    kubectl taint node <nodeName> <key>=<value>:<taint-effect>-
	
Add Toleration to a Pod:
	
apiVersion: v1
kind: Pod
metadata:
  name: nginx
  labels:
    env: test
spec:
  containers:
  - name: nginx
    image: nginx
    imagePullPolicy: IfNotPresent
  tolerations:
  - key: "key1"
    operator: "Equal"
	value:"value1"
    effect: "NoSchedule"
	

 Problem with taint and tolerations is it doesnot guarantee that the pod will be placed on a specific node.
 
 Placing pod on a specific Node :
 
 1)Using Node name :
	Using "nodeName" in pod  spec tag.
 
 2) Using Node Selectors : For this we need to label the nodes.
	
	Creating labels for a Node
	
	kubectl label node <NodeName> <key>=<value>
	Ex: kubectl label nodes node-1 size=Large
	
	Using Node selector in a pod
	
	apiVersion	: V1
	kind : Pod
	metadata:
	 name:myapp-pod
	spec:
	 nodeSelector :
		size:Large
		
	Limitaion with node selector we can add only one condition .We cannot multiple conditions add like size: large or size:medium

3) Using Node Affinity : 
	Creating labels for a Node
	
	kubectl label node <NodeName> <key>=<value>
	Ex: kubectl label nodes node-1 size=Large
	
	
	Add node Affinity
	==================
	apiVersion: v1
	kind: Pod
	metadata:
		name: with-node-affinity
	spec:
	 containers:
	  - name: nginx
	    image: nginx 
	 affinity:
	  nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchExpressions:
            - key: size
              operator: In
              values:
              - Large
			  - medium
	
	
	Supported Operators:
	In, NotIn, Exists and DoesNotExist 
	

DaemonSets :

	This will be create a pod when a new node is created.
	
	To get the daemon set
	Ex: kubectl get daemonset;
	
	To create a daemon set ( this is like creating a replicaset)  excpet the type is DaemonSet
  
	apiVersion: apps/v1
	kind: DaemonSet
	metadata:
		name: fluentd-elasticsearch
   
	
Static pods :

	Kubelet will store the pod definiton file in --pod-manifestpath="etc/kubernets/manifests" folder or --condif prop.
	This directory is passed as an argument while creating the kublet service.
	Placing pod definiton file here will allow the kubelet to create the pod.
	
	find the static pods in all namespaces
	 kubectl get pods -A ( find the pods  name which ends with naode name etc)
	 or 
	 check the owner section in pod yaml where owner reference kind=node
	   ownerReferences:
		  - apiVersion: v1
			controller: true
			kind: Node
			name: controlplane
			
	for non static pods kind: ReplicaSet
	 ownerReferences:
	  - apiVersion: apps/v1
		blockOwnerDeletion: true
		controller: true
		kind: ReplicaSet
			
	How many pod definition files are present in the manifests folder?
	
	cd /etc/kubernetes/manifests ➜  ls
	etcd.yaml  kube-apiserver.yaml  kube-controller-manager.yaml  kube-scheduler.yaml
	
	ssh to the node on which pods is avialble
	Satic pods exists at /etc/kubernetes/manifests 
	
	If not availble check the config file in  “var/lib/kublet/config.yaml” and check the static pod path 
	staticPodPath: /etc/just-to-mess-with-you
	
Configure Custom Scheduler :

  If  scheduler configured using service then check the --config in service	
  kube-scheduler.service
  ======================
	Default Scheduler :
	====================
	wget https://storage.googleapis.com/kubernetes-release/release/v1.12.0/bin/linux/amd64/kube-scheduler
	ExecStart=/usr/local/bin/kube-scheduler \\
	--config=/etc/kubernetes/config/kube-scheduler.yaml \\
	--scheduler-name= default-scheduler
	ExecStart=/usr/local/bin/kube-scheduler \\
	
	Custome Scheduler:
	=================
	--config=/etc/kubernetes/config/kube-scheduler.yaml \\
	my-custom-scheduler.service
	--scheduler-name= my-custom-scheduler
	
  If scheduler configured using static pods in static pods location change the scheduler.yaml file


Metrics  :

  Install metrics server to get the node/pod metrics.
  
  Minikube : minikube addons enable metrics-server
  Other clusters : 
	git clone https://github.com/kubernetes-incubator/metrics-server.git
	kubectl create –f deploy/1.8+/
		others
		clusterrolebinding "metrics-server:system:auth-delegator" created
		rolebinding "metrics-server-auth-reader" created
		apiservice "v1beta1.metrics.k8s.io" created
		serviceaccount "metrics-server" created
		deployment "metrics-server" created
		service "metrics-server" created
		clusterrole "system:metrics-server" created
		clusterrolebinding "system:metrics-server" created
		
		
  Node metrics : kubectl top node
  Pod metrics : kubectl top pod
  
  logs : 
  
	Display logs in a container
	
	kubectl logs –f  <POD_NAME> <CONTAINER_NAME>
	kubectl logs –f event-simulator-pod event-simulator
	
Deployment Strategies :

	1) Recreate
	2) Rolling updates - ( Default )
	
	Every deployment will have rollouts and versions
	
	>kubectl rollout status deployment/myapp-deployment
	
	>kubectl rollout history deployment/ myapp deployment
	deployments "
	myapp deployment"
	REVISION CHANGE
	CAUSE
	1 <none>
	2 kubectl apply filename=deployment definition.yml record=true

Rolling Updates :

	Create deployment with nginx:1.8
	 >kubectl create -f nginx-deployment.yaml ( nginx:1.8)
	Change the version to nginx:1.9.1 in nginx-deployment.yaml file
	 >kubectl apply -f nginx-deployment.yaml
	or
	 >kubectl set image deployment/myapp_deployment nginx =nginx:1.9.1
		

	Deployement will create a replcaset with specific image version
	When a new verison of image is  deployed a new version of replicaset is created with in deployment.

	To rollback to prev deployment version 
	 >kubectl rollout undo deployment/ myapp deployment
 
	Deployment :
    Create : kubectl create deployment nginxdeployment --image=nginx
	Get: kubectl get deployments
	Update : 
			kubectl apply -f nginxdeployment.yaml
			or
			kubectl set image deployment/nginxdeployment <containerName>=nginx:1.9.1
	status :
			kubectl rollout status deployment/myapp-deployment
	
	rollback : kubectl rollout undo deployment/myapp-deployment
	
	
 
Environment Variables :

	apiVersion: v1
	kind: Pod
	metadata:
	  name: myapp
	  labels:
		name: myapp
	spec:
	  containers:
		- name: myapp
		  image: nginx
		  resources:
			limits:
			  memory: "128Mi"
			  cpu: "500m"
		  ports:
			- containerPort: 8080
		  env:
			- name: APP_COLOR
			  value: blue
			- name: APP_MODE
			  value: production
			- name: myConfigMap
			  valueFrom:
				configMapKeyRef:
				  key: APP_LANG
				  

ConfigMaps :

apiVersion: v1
kind: ConfigMap
metadata:
  name: myConfigMap
data:
  APP_COLOR: blue
  APP_MODE: production
  APP_LANG: Java
  
 Creation :
 > kubectl create -f congimp.yaml
 or 
 >kubectl create configmap <configmap-name>  --from-literal=<key>=<value>
 
 Using configmap in pod :
	apiVersion: v1
	kind: Pod
	metadata:
	  name: confiPod
	  labels:
		name: confiPod
	spec:
	  containers:
		- name: confiPod
		  image: nginx
		  resources:
			limits:
			  memory: "128Mi"
			  cpu: "500m"
		  ports:
			- containerPort: 8080
		  envFrom:
			- configMapRef:
				name: myConfigMap


ConfigMaps in Pods :

Environment :
=============
envFrom:
- configMapRef:
	name: app-config
	
Single env variable :
=====================

env:
- name: APP_COLOR
  valueFrom:
	configMapKeyRef:
	 name: app-config
	 key: APP_COLOR
	 


Secrets:
=======

 Secrets are stored in encoded format
 
 Two steps
 1) create Secret
	- Declarative Approach
	- Imperative Approach
 2) Inject secret in to POD
	- Injecting the entire configMap
	- Injecting specific key from configMap

1) create Secret :
================ 
 Declarative :
 ===========
	apiVersion: v1
	kind: Secret
	metadata:
	  name: mysecret
	data:
	  USER_NAME: mysql
	  PASSWORD: password
	  HOST: localhost
	  
 Imperative :
 ===========
	>kubectlcreate secret generic <secret-name> --from-literal=<key>=<value>
	>kubectl create secret generic app-secret --from-literal=DB_Host=mysql --from-literal=DB_User=root
	secrets from a file :
	====================
    >kubectl create secret generic <secret-name> --from-file=<path-to-file> 
	>kubectl create secret generic app-secret --from-file=app_secret.properties 


2) Inject secret in to POD
==========================
	Injecting the entire configMap
	==============================
	envFrom:
	  - secretRef:
          name: app config
	
	Injecting specific key from configMap
	=====================================
	env:
	 - name: DB_Password
       valueFrom:
         secretKeyRef:
            name: app secret
            key: DB_Password
			


InitContainers :
================

apiVersion: v1
kind: Pod
metadata:
  name: myapp-pod
  labels:
    app: myapp
spec:
  containers:
  - name: myapp-container
    image: busybox:1.28
    command: ['sh', '-c', 'echo The app is running! && sleep 3600']
  initContainers:
  - name: init-myservice
    image: busybox:1.28
    command: ['sh', '-c', 'until nslookup myservice; do echo waiting for myservice; sleep 2; done;']



Cluster Maintainance :
======================

As part of cluseter maintainance like patching node has to stopped for software upgrade.

To move all the pods on the node to another node.

Drain pods from Node:
=====================
>kubectl drain <NodeName>

Not to Schedule pods on node :
==============================
>kubectl cordon <NodeName>

Reschede pods on cordon Node:
=============================
>kubectl uncordon <NodeName>



Upgrading Kubernetes Cluster :
==============================
Upgrading cluster involves two steps
1) Upgrading the master Node
2) Upgrading the worker Node


Show the complete plan to upgrade cluster 
	>kubeadm upgrade plan

1) upgrade the kubeadm command tool 
	>apt-get upgrade -y kubeadm=<version>
2) upgrade cluster 
	>kubeadm upgrade apply <version>
3) upgrade the kublet  
	>apt-get upgrade -y kubelet=<version>
4) Restart the kubelet service
	>systemctl restart kubelet
5) Upgade worker nodes
6) Drain the nodes on one of the worker nodes
	>kubectl drain <NodeName>
7) upgrade the kubeadm command tool 
	>apt-get upgrade -y kubeadm=<version>
8) upgrade the kubelet 
	>apt-get upgrade y kubelet=1.12.0 00
9) upgrade node 
	>kubeadm upgrade node config --kubelet-version v1.12.0
10) restart kubelet
	>systemctl restart kubelet
11) Uncordon
	>kubectl uncordon <NodeName>
	

Backup the ETCD:
================

Backup

>ETCDCTL_API=3 etcdctl --endpoints=https://127.0.0.1:2379  --cacert=<trusted-ca-file> --cert=<cert-file> --key=<key-file>  snapshot save <backup-file-location>

Restore





